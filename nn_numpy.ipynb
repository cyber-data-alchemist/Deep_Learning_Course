{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOKibLMzIE7R",
    "tags": []
   },
   "source": [
    "# Implementing a Fully Connected Network for Kaggle ASL Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r9ludP_IE7R"
   },
   "source": [
    "In this notebook, we will construct a fully connected neural network to develop a classifier for American Sign Language (ASL) hand-gesture letters. In other words, our goal is to create a model capable of interpreting a 28x28 image of a hand gesture and determining the corresponding letter. This exercise will primarily leverage numpy, avoiding the use of more powerful frameworks like TensorFlow. While this approach might have some disadvantages in terms of computing and accuracy performance, it is expected to be highly valuable for understanding the fundamentals of Deep Learning.\n",
    "\n",
    "The dataset is composed of two dataframes with the following numerical structure:\n",
    "\n",
    "\n",
    "| label | pixel1 | pixel2 | ... | pixel784 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 3 | 107 | 184 | ... | 128 |\n",
    "\n",
    "\n",
    "\n",
    "Each label corresponds to a numeric position in the alphabet, and each pixel represents the intensity in a 28x28 grid (28x28=784). During the exercise, we will implement a strategy for splitting the validation dataset into two to ultimately demonstrate the predictive capability of the model we build on unknown images through training, validation, and testing.\n",
    "\n",
    "**Let's code!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4bexwhVIE7R"
   },
   "source": [
    "We will start by calling some Python libraries for our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T41KZlfLIE7S",
    "tags": []
   },
   "source": [
    "## Calling dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S0_ryKOIE7S"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are invoking libraries to address the task of American Sign Language (ASL) Classification.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    These libraries are essential for manipulating data and implementing the functionalities needed for our task. We will use:\n",
    "    <ul>\n",
    "        <li><b>numpy:</b> It provides functions like the dot product and facilitates working with vectors, matrices, and tensors.</li>\n",
    "        <li><b>string:</b> ASL letters are represented as numeric values, and this library helps us map between them.</li>\n",
    "        <li><b>pandas:</b> We will use it to manage our dataset.</li>\n",
    "        <li><b>matplotlib:</b> It is used for creating plots or visual representations of data.</li>\n",
    "        <li><b>cv2:</b> This library aids in working with grayscale images, converting numeric values to images, and vice versa.</li>\n",
    "        <li><b>os:</b> It allows us to read the dataset stored on the local computer.</li>\n",
    "    </ul><br>\n",
    "    <b>How:</b><br>\n",
    "    We will import these libraries, which should have been previously installed using a package manager like pip or conda.\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L9Ltgc8LIE7S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws9co_8oIE7U"
   },
   "source": [
    "Now let's call the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0fFB9bJIE7U"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are crafting a function, `get_data`, to fetch the American Sign Language (ASL) dataset images. In this dataset, images are represented numerically in a tabular format. Each row corresponds to an individual image, and each column represents pixel intensity levels in grayscale, along with the associated label or letter that the image signifies.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    This function is pivotal for obtaining the data required to train our neural network to recognize ASL hand gestures. By securing a well-organized dataset, we facilitate effective learning for the model in a supervised learning setting, and illustrate the conversion of raw pixel values into comprehensible and significant information, establishing the foundation for knowledge discovery and model assessment.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    The `get_data` function primarily seeks the dataset from a local directory (`./asl_data/`). If the local search fails, the function then fetches the data from a designated GitHub repository URL. The acquired data is structured into pandas DataFrames for organized storage of images and their corresponding labels. The function yields two DataFrames: `train_df`, which houses the training dataset, and `valid_df`, which contains the validation dataset, facilitating subsequent application in model training and assessment activities.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-ROLj6wcIE7V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(gh_url=''):\n",
    "    \"\"\"\n",
    "    Retrieves MNIST datasets either from local storage or from an optional\n",
    "    GitHub URL.\n",
    "\n",
    "    Args:\n",
    "        gh_url (str): Optional parameter containing a GitHub URL where the\n",
    "        datasets are located.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: A DataFrame with the training data and\n",
    "        another with the validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining local paths\n",
    "    train_local_path = './asl_data/sign_mnist_train.csv'\n",
    "    valid_local_path = './asl_data/sign_mnist_valid.csv'\n",
    "\n",
    "    # Defining GitHub paths\n",
    "    train_gh_url = f'{gh_url}sign_mnist_train.csv'\n",
    "    valid_gh_url = f'{gh_url}sign_mnist_valid.csv'\n",
    "\n",
    "    # Checking if files are available locally\n",
    "    train_exists = os.path.exists(train_local_path)\n",
    "    valid_exists = os.path.exists(valid_local_path)\n",
    "    local_files_available = train_exists and valid_exists\n",
    "\n",
    "    # If local files are available, load them; otherwise, load from GitHub\n",
    "    if local_files_available:\n",
    "        train_df = pd.read_csv(train_local_path)\n",
    "        valid_df = pd.read_csv(valid_local_path)\n",
    "    else:\n",
    "        train_df = pd.read_csv(train_gh_url)\n",
    "        valid_df = pd.read_csv(valid_gh_url)\n",
    "\n",
    "    # Returning training and validation DataFrames\n",
    "    return train_df, valid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "J300nYDMIE7V",
    "outputId": "50ee79a1-8dd6-42d9-98bc-cb3b16d9d4eb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Train dataset shape (27455, 785)\n",
      "Validation dataset shape (7172, 785)\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_df, valid_df = get_data('/')\n",
    "\n",
    "# Visualize shapes and head of trainning\n",
    "print('------')\n",
    "print(f'Train dataset shape {train_df.shape}')\n",
    "print(f'Validation dataset shape {valid_df.shape}')\n",
    "print('------')\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1hg_8hJIE7V"
   },
   "source": [
    "Results above display the expected data structure of images by rows, with labels and pixels representing the values of their grey intensity. Since we will be manipulating the datasets beyond this point, we will store some original copies in case the unaffected datasets are needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "heDeY7waIE7V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store copies of original DataFrames\n",
    "train_df_original = train_df.copy()\n",
    "valid_df_original = valid_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOJSQKzZIE7W"
   },
   "source": [
    "Next, we will define a function to split the validation dataset into two; this will result in six datasets: training, validation, and testing, where each will have an x version (features) and a y version (labels).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brDvTyCIIE7W"
   },
   "source": [
    "### Import images function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzZf8Nf4IE7W"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We're segregating our data into features (x) and labels (y) to facilitate model training. We create a training set to instruct the model, a validation set to verify its learning accuracy, and a test set to assess its predictive capability on unseen data.\n",
    "    <ul>\n",
    "        <li><b>x_train and y_train:</b> They contain the training features and labels.</li>\n",
    "        <li><b>x_val and y_val:</b> They hold the features and labels used for validating our training.</li>\n",
    "        <li><b>x_test and y_test:</b> They encompass the features and labels of unseen data to test our model.</li>\n",
    "    </ul>\n",
    "<br>\n",
    "    <b>Why:</b><br>\n",
    "    While our data is initially divided into training and validation sets, having an additional test set enables us to ensure that our model generalizes well to new, unseen data. Employing a function for this task aids in maintaining objectivity and ensuring our model is equitable and proficient in managing uncertainty.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    We extract labels from the 'label' column, with the remaining columns serving as our features. We introduce a function, <b>split_val_test</b>, allowing us to customize the size and optionally shuffle the data, enabling the bifurcation of our validation set into two: one for validation and one for testing. This function maintains the integrity and order of the data, yielding precise subsets for validation and testing.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GQ0V4WqJIE7W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''\n",
    "    Create a function that will allow you to split the previously loaded\n",
    "    validation set into validation and test.\n",
    "\n",
    "    Args:\n",
    "      x (np.array): Input array for predictors; it must be a 2D array.\n",
    "      y (np.array): This should be a 1D numpy array with the same length\n",
    "                    as x.shape[0].\n",
    "      pct (float): Should be between 0.0 and 1.0 and represent the proportion\n",
    "                   of the dataset to include in the test split (x_test).\n",
    "      shuffle (bool): Decides whether to shuffle the rows of x and y arrays.\n",
    "\n",
    "    Returns:\n",
    "      x_val (np.array): Output array containing the predictors with\n",
    "                        x.shape[0]*(1-pct) records for the validation set.\n",
    "      y_val (np.array): Output array containing the labels with\n",
    "                        x.shape[0]*(1-pct) records for the validation set.\n",
    "      x_test (np.array): Output array containing the predictors with\n",
    "                         x.shape[0]*pct records for the testing set.\n",
    "      y_test (np.array): Output array containing the labels with\n",
    "                         x.shape[0]*pct records for the testing set.\n",
    "    '''\n",
    "    # Arguments validation\n",
    "    ## Check correct pct value\n",
    "    if pct < 0 or pct > 1:\n",
    "        raise ValueError(\"Invalid input: 'pct' should be between 0.0 and 1.0\")\n",
    "    ## Check y dimensionality\n",
    "    if y.ndim != 1:\n",
    "        raise ValueError(\"y should be 1D numpy array\")\n",
    "    ## Check x and y has the same number of rows\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"Invalid input: x and y arrays should have the same \"\n",
    "                         \"amount of rows\")\n",
    "\n",
    "    # Get the number of rows equivalent to pct selected\n",
    "    test_nrows = int(x.shape[0] * pct)\n",
    "\n",
    "    # Shuffle the values (if option selected)\n",
    "    if shuffle:\n",
    "        ## Generate random indices for shuffling\n",
    "        random_indices = np.random.permutation(x.shape[0])\n",
    "        x = x[random_indices, :]\n",
    "        y = y[random_indices]\n",
    "\n",
    "    # Select the first N - test_nrows rows (N = total number of records)\n",
    "    # for the validation set\n",
    "    x_val = x[-test_nrows:, :]\n",
    "    y_val = y[-test_nrows:]\n",
    "\n",
    "    # Select the last test_nrows of the arrays for the testing set\n",
    "    x_test = x[:-test_nrows, :]\n",
    "    y_test = y[:-test_nrows]\n",
    "\n",
    "    return x_val, y_val, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9F3HbZ4IE7W"
   },
   "source": [
    "Although we could call the function right now, we choose not to do so just yet. This is because there are still some data transformations required, including normalization and the removal of the letters <b>J</b> and <b>Z</b>, as these gestures involve movement, which we cannot capture with a static-image dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKKC544qIE7W"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We will omit <b>J</b> and <b>Z</b> from our dataset.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    Signs <b>J</b> and <b>Z</b> inherently involve movement, and since our dataset consists of static images, these movements aren't captured, making them incompatible with our model’s scope. Our neural network is conceived to interpret ASL from static images representing hand gestures, and signs like <b>J</b> and <b>Z</b> which involve motion, don’t conform to this criterion [1].\n",
    " <br><br>\n",
    "    <b>How:</b><br>\n",
    "    We will isolate and remove the rows corresponding to these letters in our datasets. Given that each label is numerically encoded based on the ascii_lowercase list, we will find the numeric representations of these letters, locate the corresponding rows in the labels dataset, identify and remove them. This will maintain the alignment between the features and labels in our datasets, omitting the rows related to <b>J</b> and <b>Z</b>.\n",
    "<br>\n",
    "</div>\n",
    "\n",
    "![test_image](https://www.lingvano.com/asl/wp-content/uploads/sites/3/2022/11/Sign_alphabet_chart_abc.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oRKHqV8xIE7X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_datasets(train_df_original, valid_df_original):\n",
    "    \"\"\"\n",
    "    Excludes rows with labels corresponding to 'j' and 'z' from the training\n",
    "    and validation datasets.\n",
    "\n",
    "    Args:\n",
    "        train_df_original (pd.DataFrame): The original training dataset.\n",
    "        valid_df_original (pd.DataFrame): The original validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: The filtered training and validation\n",
    "                                    datasets.\n",
    "    \"\"\"\n",
    "    # Finding the keys corresponding to 'j' and 'z' in the alphabet dictionary\n",
    "    \n",
    "    reverse_alphabet_dict = {letter: i for i, letter in enumerate(string.ascii_lowercase)} \n",
    "    j_key = reverse_alphabet_dict.get('j')\n",
    "    z_key = reverse_alphabet_dict.get('z')\n",
    "\n",
    "    # Filtering out rows with labels 'j' and 'z' from the original training\n",
    "    # and validation datasets\n",
    "    condition_train = (\n",
    "        (train_df_original['label'] != j_key) &\n",
    "        (train_df_original['label'] != z_key)\n",
    "    )\n",
    "    filtered_train_df = train_df_original[condition_train]\n",
    "\n",
    "    condition_valid = (\n",
    "        (valid_df_original['label'] != j_key) &\n",
    "        (valid_df_original['label'] != z_key)\n",
    "    )\n",
    "    filtered_valid_df = valid_df_original[condition_valid]\n",
    "\n",
    "    return filtered_train_df, filtered_valid_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECSucZmmIE7X"
   },
   "source": [
    "The function above will assist us with the task at hand, allowing us to remove the specified letters from both datasets to prevent data leakage (having some images in the validation and test datasets that were not observed in the training one).\n",
    "\n",
    "Next, we will proceed to create a normalization function that will help ensure the variables are on similar scales, mitigating issues such as varying light exposure that might result in different shades of grey in some images. This normalization will be based on z-scale, and we aim to transform the data to have a mean of 0 and a standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63WjWyQuIE7X"
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9G_W881IE7X"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We will to normalize the features.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    Normalizing the features allows the model to interpret them based on their variation, not their raw values, avoiding any undue influence from features with larger numeric values. This process ensures a fair contribution from each feature to the model's performance by transforming them to have a mean of 0 and a standard deviation of 1 through Z-Scale normalization.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    We will use Z-Scale transformation for normalization. To avoid data leakage, the mean and standard deviation of the training dataset will be used to normalize the other datasets. Specifically, we will subtract the mean from every numeric value and then divide by the standard deviation, ensuring consistent and unbiased scaling across all datasets.\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Fo8wAc-EIE7X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the Z-score normalization function\n",
    "def normalize(x_mean, x_std, x_data):\n",
    "    \"\"\"\n",
    "    Applies a Z-Scale transformation to a numpy array using provided x_mean and x_std.\n",
    "\n",
    "    Args:\n",
    "    x_mean (float): The mean value to be used for normalization.\n",
    "    x_std (float): The standard deviation to be used for normalization.\n",
    "    x_data (np.array): The array to which the normalization will be applied.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The normalized array.\n",
    "    \"\"\"\n",
    "    return (x_data - x_mean) / x_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-03CO5koIE7Y"
   },
   "source": [
    "### Visualize samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4BF6oCnIE7Y"
   },
   "source": [
    "To better understand the problem, we will visualize some images. This will give us a clearer idea of the kind of data we are working with and what our Neural Network aims to solve. This step is primarily intended to develop an intuition about the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObUavCzXIE7Y"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We will visualize some examples in the training dataset.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    Visualizing examples enhances our understanding of the classification task of letters in ASL from images of hand postures. By seeing the representation of different letters through various hand postures, we can intuitively comprehend how the numeric values in the dataframe correspond to different images. This insight is crucial, as it validates the potential of using these numeric values in a Neural Network to generate meaningful results.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    We will define a function to visualize a row from the dataframe, representing a letter in the ASL alphabet. This function will reshape the linear pixel values to a square image and use matplotlib to display it in grayscale, allowing us to interpret the numeric data as images.\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "9f7_zudIIE7Y",
    "outputId": "39028c6f-19b6-4f05-cd38-f79ab61f691a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shown corresponds to the following value: c\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAGVCAYAAADZmQcFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARSUlEQVR4nO3cy4uWBf8G8HuOzozjIR0nk8ySzDIJi4gQokCCaNGiKKIo2rVp06K/o3+i9rWsBBdGtCgIIgIzNG08jOl4mIPOyXf3wvvbvPd9vd+5O/w+n3VX32eeeZ65vDfXwN27d+82APA/GvyzXwAA/wwKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEsNt/8PPP/98I19HiYGBgSiXjgWk9/4O1tfX/+yX8F/1+f6n70ff7+PgYPd/I66trUW30vd/dXU1yiXf07GxsejWiRMnotwnn3wS5aanpztn0r9bk5OTUe677777r/+NJxQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASrReGx4ZGYkO9Lm2miytNk3+GpN7fS8i9/mz/ZOlv7f0e5O+/8vLy50z6c82NDQU5VLJdyB9H/ft2xflktXgpmma4eHWf4r/bWlpKbq1sLAQ5drwVwOAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASrRfJ0iG4JNf3oGH6s6Wjeol0HDJ9jX2OQ6avcW1trbfc+Ph4dOv69etRbnFxMcrt3r27cyZ9H/v+nib30u/N3r17o1zy/jdN08zOznbOJIOSTZOPSrbhCQWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEhu+Npysfaa3UukiaZ+vM1127fM1piuyq6urUW5kZCTKTU5Ods7Mzc1Ft77//vtec88//3znzJ07d6JbDz30UJTbv39/lJufn++cSb83yWekaZpm586dUe7MmTOdM+mS9ejoaJRrwxMKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACX+kmvDAwMD0a107Tb1d/jZ0ntra2u93UqXXdOV3J9++qlz5ttvv41uXbp0KcrNzs5GuePHj3fOpJ+tH3/8Mcq99tprUW56erpzZmVlJbqV/r17+OGHo9zJkyc7Z5aWlqJbu3btinJteEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoETrteGRkZGNfB1skGQRuWmaZtOmTZ0zmzdvjm6tr69HuS+++KK3XLpsPD8/H+VSyXs5Ojoa3Tpz5kyUm5iYiHLvv/9+50z62UoXmA8dOhTlVldXO2eWl5ejW5cvX45ybXhCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoETrcch0LC2R3hoYGIhy6YBiIhmBa5r8PUnH8SYnJztnVlZWolvHjx+PcidPnoxyV69ejXJ9unHjRpRbXFzsnNm+fXt0K5UOzY6NjXXOpKOeQ0NDUS4ZVW2apnnrrbc6Z9K/W19//XWUa8MTCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlWq8NDw+3/k//Z+mKZroQ2qe+V4NTu3bt6pz58ssvo1ufffZZlJudnY1yyZJvupCb5tLl7LW1tc6ZdJF3z549Ue7YsWNRLvm7kK7/pt+33bt3R7nk933w4MHo1ksvvRTl2vCEAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkCJ1hPCfS75pmvD6UJrn9LV5sXFxSi3ffv2KLe8vNw58+uvv0a3+l5SHh8f75xJF3knJiai3NjYWJR75plnOmeeeuqp6Nbhw4ej3JYtW6LcyspK50z6dyvNJWvPTdM0v/32W+fMu+++G93ayO+bJxQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASrSevh0czLonyfW9PpuuGyfSReTR0dEol64Nnzp1qnPm3Llz0a35+fkoly7yJrl07TldG3711Vej3Ouvv945c/v27ehW+r1Jl5v7lP69S9eGH3/88c6Z9O/kN998E+XaLFl7QgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaBE63HIPg0NDf3ZL6GVZBwvHY/bunVrlEsH/C5cuNA5k44Mrq6uRrk+7dixI8p9+OGHUe6JJ56IcsmIZToymH5P06HTRPp9S7836XvywgsvdM6cOHEiunX69Oko14YnFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKtF4bHh7OhokHBgY6Z9KlzzQ3OJj1apJLl13TtdulpaUol6zWpiuy9913X5S7fPlylFteXu6cOXDgQHTr2WefjXLXr1+Pcsnabfr5T77bTZN/TxPp9y01MjIS5aampjpn7rnnnujWwYMHo1wbnlAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKLHha8OJdI00XRZN11YTExMTUW7btm1RbmZmJsrNzc11zszPz0e3koXcpsnXVs+fP985s3///uhW+tlK35M+171Ta2trvd1K38e+l8uTBezHHnssurW6uhrl2vCEAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkCJDZ8QTtZP08XONJe8xqZpmjt37nTObN26NbqVunnzZpRbXFzsnEnXhm/fvh3l0kXYqampzpmnn346upVKV3IT6fuY5vr82fpeIE/vJWvuyd+fpmmazZs3R7k2PKEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQovUiWTqWlg7I9Sl9jSsrK50zO3bsiG7dunWr11wyPJd+RpaWlqJcOjI4Pj7eOTM9PR3dSscC+xxQXFtb6+1W0+Sfk7/6rabp9/edjjzOzs5GuTY8oQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQovXacJ/rp6mBgYEot7i4GOUmJyc7Z8bGxqJb586di3Jzc3NRbn5+vnMmXa0dGRmJculKdHJvy5Yt0a2/w9p239/tPt+TvpeUh4db/0n9D8l7ki4p33vvvVGuDU8oAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRoPY2ZLvmmuT6trq5GuQcffLBzZnl5Obp148aNXnPJ2vD6+np0K7WwsBDlpqamOmemp6ejW9evX49yf4fvTSr92ZIl3/S7nX5P03Xj8fHxzpm/4pK1JxQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASmz42nCf0qXPwcGsVycnJztnrl69Gt1Kl3XTXLK2urKyEt1Klo2bJl83npmZ6Zz56quvolvHjh2Lcjdv3oxyyWc5/fyn0t/3xYsXO2fSte1r165FuTt37kS5ZBX5yJEj0a2dO3dGuTY8oQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFCi9Thkn0ZGRqJcOjo3PT0d5RLp6N+tW7d6zSVjdekw3uLiYpTbsmVLlEtGPT/++OPo1qZNm6Lciy++GOWSUcMrV65Et5KxxqbJB1KTgdqxsbHoVvpZvnz5cpT74YcfOmfOnTsX3frggw+iXBueUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAoseFrw0NDQ50z6dJnaufOnVEueZ23b9+ObiUrsk3T79pwuvac3GqafEk2WbPetm1bdOvTTz+NcumS7+Bg938jpp/JdKV769atUS5ZpT516lR0a2ZmJsrt27cvyr3yyiudM/fff390K/19t+EJBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASrdeGk9XgpsmWXa9fvx7d2rNnT5QbHR2NcnNzc50z6c+Wrgany80rKyudM+mKabo+OzU1FeWS3/fk5GR0K11SPnnyZJTbu3dv58yjjz4a3RoezsbKz549G+V+/vnnzpn0e/PGG29EuSeffDLKJX8n19bWolvLy8tRrg1PKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUaD0XmqxhNk3TLC4uds5s3749urVjx44oly7yJkum8/Pz0a10yTfNJUumY2Nj0a2dO3dGuXSleGJionMmXdZNF2EHB7N/6yW/g3PnzkW3Ll26FOUGBgai3O7duztn3nnnnejWoUOHoly6bpx8T+/evRvd2kieUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACjRevFuZWUlOrC8vNw5s3fv3uhWOvK4sLDQWy69lY7O3bhxI8olo4bJ6GLT5GOg6b1kQHF9fT261feo54ULFzpntmzZEt06evRolHvuueei3PT0dOfM6upqdCsdcU1HPdPBzET6WW7DEwoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJVqvDaertTt27OicSZeN01y67Hrz5s3OmUuXLkW3rl69GuXSZdG7d+92zoyPj0e30rXhkZGRKJcs0C4tLUW3rl27FuXStdsjR450zrz33nvRrQceeCDKpe9luiaeGB5u/afxPyTfm6bZ2AXg/ytdRG71/96w/zMA/68oFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEq0ntTctm1bdCBZhE3XSJP136ZpmosXL0a5X375pXPm7Nmz0a25ubkol66YJsvNk5OT0a30NSarwU2T/Wznz5+Pbu3duzfKpQvAR48e7ZxJV5vT72n6+04WgNP131Sfq8GpjXxPPKEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUKL1fOfQ0FB0YHl5uXNmcXExupWuBv/+++9R7o8//uicWVhYiG4lC7lN0zRra2tRbnR0tHMmXa1NPiNNk78nly9f7pw5cOBAdOujjz6Kcrt27YpyyXcnfR8HBgaiXJ/Sv1vp9yZ9T5LX2feSchueUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACjRehxyfX09OpAM/125ciW6dfr06Sg3NzcX5ZLXOTs7G91Kx+rScbxNmzZ1zqSfkfT9TwYsm6ZpXn755c6ZN998M7o1MTER5e7cuRPlhodbf6X/LR0ZTHODg/39Ozb9TKYjj30OZvb9s7XhCQWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEq2nSVdWVqIDCwsLnTM3b96Mbl24cCHKzczMRLlr1651zty4cSO6la7WJuuzTZP/vhOPPPJIlHv77bej3OHDhztnbt++Hd1KV6LT31u6QPtPlS4bp7n0/U+Wm9O1543kCQWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgN3/4qTlQD87XhCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoMS/AK9dRqQRXUBaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_letter(image, letter):\n",
    "    \"\"\"\n",
    "    Plots the given image in grayscale and displays the corresponding letter.\n",
    "\n",
    "    Args:\n",
    "    image (np.array): 1D array representing the image to be plotted.\n",
    "    letter (str): The letter corresponding to the image.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(image.reshape(28,28), cmap=plt.get_cmap('gray'))  # Assuming the image is 28x28 pixels\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Reconstructing the alphabet dictionary\n",
    "alphabet_dict = {idx: letter for idx, letter in enumerate(string.ascii_lowercase)}\n",
    "\n",
    "# Extracting y and x from the train_df for a sample visualization\n",
    "y_train_sample = train_df['label']\n",
    "x_train_sample = train_df.drop(columns=['label'])\n",
    "\n",
    "# Applying the plotting function to a random sample from the training dataset.\n",
    "rand_idx = np.random.randint(len(y_train_sample))\n",
    "selected_letter = alphabet_dict[y_train_sample.iloc[rand_idx]]\n",
    "print('The image shown corresponds to the following value:', selected_letter)\n",
    "plot_letter(x_train_sample.iloc[rand_idx].values, selected_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vm4V5DSIE7Y",
    "tags": []
   },
   "source": [
    "## Create a Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF-jZHA_IE7Y"
   },
   "source": [
    "As mentioned, we will create a fully connected neural network using almost exclusively numpy. Our aim is to construct a method to have a vertical line of neurons that can forward their values and perform backpropagation. We will measure their errors and loss function by comparing the outcomes to the true label information and will use ReLU to introduce non-linearity to our linear calculations, enabling us to better solve the classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17c3Z53gIE7Y"
   },
   "source": [
    "### Our model equations\n",
    "\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$ ... $$\n",
    "\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHdtE1GvIE7Y"
   },
   "source": [
    "### Adittional Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHQBM3SDIE7Z"
   },
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJi8qPL_IE7Z"
   },
   "source": [
    "We will begin by writing code to divide the information into batches. This will allow us to process the large dataset in smaller chunks and will also implement a shuffle functionality to ensure fairness in the distribution of the chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXuIqrWsIE7Z"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are constructing an iterable object comprising subsets of images and labels.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    Processing the entire dataset at once during training is often not feasible due to memory constraints. By utilizing subsets or mini-batches, we can efficiently train the model. This object will contain random samples from the dataset to avoid biases associated with the order of the labels, ensuring that each batch is representative and doesn't over-specialize in a specific subset of the data.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    We will implement a function that takes features and labels, shuffles them optionally, and then divides them into ‘k’ subsets. This function yields smaller batches of x and y, aiding in training our neural network incrementally. Employing mini-batches allows the cost function to average the error over the samples within each batch, fostering stable and generalized learning.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hEP9QWVjIE7Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle=True):\n",
    "    '''\n",
    "    Function that converts the dataset into an iterator (generator) allowing to\n",
    "    work with the dataset in chunks.\n",
    "\n",
    "    Args:\n",
    "      mb_size (int): Number of rows to split the dataset, the batch size.\n",
    "      x (np.array): Predictors matrix; must have 784 fields.\n",
    "      y (np.array): Labels vector; must be a 1D array.\n",
    "\n",
    "    Returns:\n",
    "      generator: An iterator that generates minibatches of data.\n",
    "    '''\n",
    "    \n",
    "    # Verify that predictor and label sets have the same amount of records\n",
    "    assert x.shape[0] == y.shape[0], 'Row count mismatch'\n",
    "\n",
    "    # Get total number of records in the set\n",
    "    total_data = x.shape[0]\n",
    "\n",
    "    # Evaluate if shuffle option is active\n",
    "    if shuffle:\n",
    "        # Create an array of sequential numbers to simulate indexes\n",
    "        idxs = np.arange(total_data)\n",
    "        \n",
    "        # Shuffle the values that represent the indexes\n",
    "        np.random.shuffle(idxs)\n",
    "        \n",
    "        # Change the order of the rows in x and y the same way\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]\n",
    "\n",
    "    # Return a generator (an object type iterator), that iterates reading the\n",
    "    # entire set in sets with length == to the mb_size\n",
    "    return ((x[i:i + mb_size], y[i:i + mb_size]) for i in range(0, total_data, mb_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl1AMTQ-IE7Z"
   },
   "source": [
    "## Definee Tensor, Linear, ReLU and Sequential Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08wK8uxQIE7Z"
   },
   "source": [
    "Moving forward, we will refer to most of our objects as tensors. We will create a class to facilitate this transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p56LHMPaIE7d",
    "tags": []
   },
   "source": [
    "###  Tensor Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqg8qbtkIE7d"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are creating a class named `np_tensor`.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    This class will be used in subsequent classes to ensure that the values are NumPy ndarrays, capable of performing the required mathematical operations.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    Since `np.ndarray` already possesses all the properties we require, our class will simply inherit from it without adding any additional functionality.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zkXJbR6YIE7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a new class derived from numpy ndarray to allow the assignment of attributes\n",
    "class np_tensor(np.ndarray):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKLLSC34IE7e"
   },
   "source": [
    "###   Linear Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-_lnntuIE7e"
   },
   "source": [
    "Here, we will define our linear layer along with the neurons; these neurons will require an input size and an output size. This layer will feature forward and backpropagation methods and essentially represents:\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GeWriJWIE7e"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are constructing a class, `Linear`, representing a linear layer for the neural network, corresponding to \\(Z = WX + b\\). It visually represents a vertical layer in the network and is formulated to perform a linear transformation on the input data.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    The linear layer is a foundational component in constructing neural networks and serves as a basic unit that modifies the input data. It plays a crucial role in learning representations within the network, serving as a precursor to activation functions and facilitating both forward and backward propagation within the network.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    Our class will encompass the following components:\n",
    "    <ul>\n",
    "        <li><b>__init__(self, input_size, output_size):</b> Initializes the weights 'W' and bias 'b' utilizing Kaiming He initialization [2], considering the provided input and output sizes.</li>\n",
    "        <li><b>__call__(self, X):</b> Executes the forward pass, computing \\( Z = WX + b \\).</li>\n",
    "        <li><b>backward(self, X, Z):</b> Calculates the gradients concerning the input, weights, and bias during the network's backward pass.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SH36factIE7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Initialize parameters using Kaiming He initialization.\n",
    "\n",
    "        Args:\n",
    "          input_size (int): Number of nodes in the previous layer or the\n",
    "                            number of pixels in the input.\n",
    "          output_size (int): Number of nodes in the current layer.\n",
    "        '''\n",
    "        \n",
    "        # Get the initial values for weights matrix in the layer.\n",
    "        # The weights matrix is of dimensions: nodes in the current layer X\n",
    "        # nodes in the previous layer, as this value will be multiplied by\n",
    "        # the input matrix (X) from the right.\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
    "\n",
    "\n",
    "        # Get the initial value for bias vector with zeros.\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        '''\n",
    "        Apply the linear function using the weights and biases to perform\n",
    "        feedforward computation.\n",
    "\n",
    "        Args:\n",
    "          X (np.array or np_tensor): Input data.\n",
    "\n",
    "        Returns:\n",
    "          np.array or np_tensor: Result of the linear transformation.\n",
    "        '''\n",
    "        # Feedforward: Applying linear function using the weights and biases.\n",
    "        Z = self.W @ X + self.b\n",
    "        return Z\n",
    "\n",
    "    def backward(self, X, Z):\n",
    "        '''\n",
    "        Calculate the backward propagation of the error (gradient).\n",
    "\n",
    "        Args:\n",
    "          X (np.array or np_tensor): Input data.\n",
    "          Z (np.array or np_tensor): Intermediate computed value during\n",
    "                                     feedforward.\n",
    "\n",
    "        Returns:\n",
    "          None: The function updates the gradients in-place.\n",
    "        '''\n",
    "        # Calculate the gradient\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        self.W.grad = Z.grad @ X.T\n",
    "        self.b.grad = np.sum(Z.grad, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq9dER2pIE7f"
   },
   "source": [
    "### ReLU Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6B4INVAIE7f",
    "tags": []
   },
   "source": [
    "To remove linearity and address the vanishing gradient problem, we will implement a ReLU layer. This layer will take the input from a linear layer and transform their values to the maximum between 0 and their value (effectively dropping negative values). This layer will also have forward and backpropagation methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JXp-qDiIE7f",
    "tags": []
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are developing a class, `ReLU`, representing the Rectified Linear Unit (ReLU) activation function.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    The ReLU activation function is integral in neural networks due to its efficacy in addressing the vanishing gradient problem, enabling the learning process in deeper networks. By introducing non-linearity, it allows the network to adapt and learn complex patterns, making adjustments based on the learned error [3].\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    The `ReLU` class will have methods to perform the ReLU operation, replacing negative input values with zero while maintaining positive values. It will also contain a method to compute the gradient of the ReLU function during backpropagation, which is crucial for efficient weight updates in the neural network.\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U0aoukbZIE7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, Z):\n",
    "        '''\n",
    "        Apply the ReLU function, copying the same input value if positive or\n",
    "        setting to 0 if negative.\n",
    "\n",
    "        Args:\n",
    "          Z (np.array or np_tensor): Input data.\n",
    "\n",
    "        Returns:\n",
    "          np.array or np_tensor: Result of applying the ReLU function.\n",
    "        '''\n",
    "        # Apply ReLU function, copying the same input value if positive or\n",
    "        # setting to 0 if negative.\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z, A):\n",
    "        '''\n",
    "        Compute the gradient for the ReLU function during backpropagation.\n",
    "\n",
    "        Args:\n",
    "          Z (np.array or np_tensor): Intermediate computed value during\n",
    "                                     feedforward.\n",
    "          A (np.array or np_tensor): Output of the activation function.\n",
    "\n",
    "        Returns:\n",
    "          None: The function updates the gradients in-place.\n",
    "        '''\n",
    "        # The slope of the activation function is 1, set its output gradient\n",
    "        # the same as the function output node (now used as input in backward\n",
    "        # propagation).\n",
    "        Z.grad = A.grad.copy()\n",
    "\n",
    "        # If the grad of the reversed input is lower than 0, set activation\n",
    "        # function slope as 0.\n",
    "        Z.grad[Z <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54uC_N7uIE7f"
   },
   "source": [
    "### Sequential Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoo-UrrbIE7f"
   },
   "source": [
    "Now that we have a method to generate the layers, we will create a coordinator for the full neural network, which we will call the 'Sequential' class. This will facilitate communication between the layers. The Sequential class will not only conclude with a predictor but will also manage the forward and backpropagation methods; it is here that we will define the learning rate, which will influence how much the gradients adjust the weights and biases values during backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umIEIj8UIE7g"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are developing a class, `Sequential_layers`, designed to manage and organize a sequence of layer objects, such as Linear and ReLU, in a structured manner to facilitate the streamlined execution of the neural network.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    The structured organization of layers is paramount for ensuring seamless data flow through the network, allowing for efficient building, training, and execution of neural network models. This class plays a crucial role in managing forward and backward propagation through the network layers, serving as a conduit for data between them.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    The `Sequential_layers` class will implement the following methods:\n",
    "    <br>\n",
    "<ul>\n",
    "<il><b>__init__(self, layers):</b> Initializes the class with structures to hold inputs and outputs at each layer, accepting a list of layer objects.</il>\n",
    "\n",
    "<il><b>__call__(self, X):</b> Manages a forward pass through the sequence of layers, storing the intermediate outputs at each step.</il>\n",
    "\n",
    "<il><b>backward(self):</b> Coordinates the backward pass through the sequence of layers in reverse order, calculating the gradients at each step.</il>\n",
    "\n",
    "<il><b>update(self, learning_rate):</b> Updates the weights and biases of each layer in the sequence based on the computed gradients and a specified learning rate.</il>\n",
    "\n",
    "<il><b>predict(self, X):</b> Executes a forward pass through the network and returns the model’s prediction as the index of the maximum value in the output.</il>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5a592GIvIE7g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sequential_layers():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Initializes the class with structures to hold inputs and outputs\n",
    "        at each layer, accepting a list of layer objects.\n",
    "\n",
    "        Args:\n",
    "            layers (list): List containing objects of type Linear, ReLU.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.x = None\n",
    "        self.outputs = {}\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Manages a forward pass through the sequence of layers, storing the\n",
    "        intermediate outputs at each step.\n",
    "\n",
    "        Args:\n",
    "            X (np.array or np_tensor): Input data.\n",
    "\n",
    "        Returns:\n",
    "            np.array or np_tensor: The result after passing through all the layers.\n",
    "        \"\"\"\n",
    "        self.x = X\n",
    "        self.outputs['l0'] = self.x\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            self.x = layer(self.x)\n",
    "            self.outputs['l' + str(i)] = self.x\n",
    "        return self.x\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Coordinates the backward pass through the sequence of layers in reverse\n",
    "        order, calculating the gradients at each step.\n",
    "        \"\"\"\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.outputs['l' + str(i)], self.outputs['l' + str(i + 1)])\n",
    "\n",
    "    def update(self, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of each layer in the sequence based on\n",
    "        the computed gradients and a specified learning rate.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): The learning rate used to update the parameters.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLU): continue\n",
    "            layer.W = layer.W - learning_rate * layer.W.grad\n",
    "            layer.b = layer.b - learning_rate * layer.b.grad\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Executes a forward pass through the network and returns the model’s\n",
    "        prediction as the index of the maximum value in the output.\n",
    "\n",
    "        Args:\n",
    "            X (np.array or np_tensor): Input data.\n",
    "\n",
    "        Returns:\n",
    "            int: Index of the maximum value in the output.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.__call__(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaG1wmYkIE7g"
   },
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMaMlRXGIE7g"
   },
   "source": [
    "To assess the performance of our network, we will create a cost function using a softmax of the output and comparing that output to the real known values; we will use this as a loss function. Additionally, we are implementing an accuracy function to determine how many values were predicted accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2PmMbt7IE7g"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are implementing the cross-entropy cost function, which utilizes softmax to quantify the dissimilarity between the model’s predictions and the actual labels.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    The cross-entropy cost function is crucial for evaluating the model's performance during training, highlighting how well the model’s predictions match the true labels. Minimizing this function is essential for enhancing the model's predictive accuracy.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    We transition from an ideal state, where the probability of the true class is 1, and for other classes, it is 0, to applying the softmax operation to convert raw model outputs into probabilities. The computed cross-entropy loss represents the dissimilarity between the ideal and predicted probability distributions. The process involves:\n",
    "    <ul>\n",
    "        <li>Calculating the exponential of each score and normalizing them to obtain probabilities.</li>\n",
    "        <li>Evaluating the cost by assessing the negative log probabilities attributed to the true classes and averaging over the batch size.</li>\n",
    "        <li>Deriving the gradient of the loss concerning the raw scores (pre-softmax) for backpropagation.</li>\n",
    "    </ul>\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Qx4iEJ30IE7h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the predicted values from the network and\n",
    "    returns the predictions and the computed cost.\n",
    "\n",
    "    Args:\n",
    "      x (np.array): The network's raw output (logits).\n",
    "      y (np.array): True class labels.\n",
    "\n",
    "    Returns:\n",
    "      np.array: The predicted probabilities after applying softmax.\n",
    "      float: The computed cross-entropy cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of columns (classes) of the input\n",
    "    batch_size = x.shape[1]\n",
    "    \n",
    "    # Apply the function e^x to each element (class) of the input classes\n",
    "    # predicted by the model\n",
    "    exp_scores = np.exp(x)\n",
    "    \n",
    "    # Convert the class inputs into probabilities based on all the classes\n",
    "    # (softmax function)\n",
    "    probs = exp_scores / exp_scores.sum(axis=0)\n",
    "    preds = probs.copy()\n",
    "    \n",
    "    # Cost\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    \n",
    "    # Calculate the gradients generated from the cost\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1  # dl/dx\n",
    "    x.grad = probs.copy()\n",
    "\n",
    "    return preds, cost\n",
    "\n",
    "\n",
    "def accuracy(mb_size, x, y):\n",
    "    \"\"\"\n",
    "    Calculates the general accuracy of a model.\n",
    "\n",
    "    Args:\n",
    "      mb_size (int): Number of rows to split the dataset; the batch size.\n",
    "      x (np.array): Validation or testing input fields for making predictions.\n",
    "      y (np.array): Corresponding labels for the validation or testing fields.\n",
    "\n",
    "    Returns:\n",
    "      float: The calculated accuracy of the model.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # We use a for loop as we utilize the minibatches function which obligates\n",
    "    # to split the dataset\n",
    "    \n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y)):\n",
    "        # We run the model (feedforward) to get predictions\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "    \n",
    "        # If the index of the predicted class with the index of the real value\n",
    "        # class, sum in the correct sum\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
    "        \n",
    "        # Continue adding in order to get the total number of records\n",
    "        # (this value would be equivalent to x.shape[0])\n",
    "        total += pred.shape[1]\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izzUTuFwIE7h"
   },
   "source": [
    "### Trainning Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2drMRR5IE7h"
   },
   "source": [
    "Our model requires training to begin generating results. Each result will produce a loss, enabling us to use the backpropagation method to refine our values. After all, we do start with an initial set of values in our neurons, but those values can be updated over time. This illustrates the need to establish the training loop, where we will specify the number of epochs, the batch size, and the learning rate of our Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PTnP0caIE7h"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We are defining a function, 'train', to facilitate the training of our model, returning both loss and accuracy for each epoch.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    Training the model is a pivotal step in building neural networks, and having a dedicated function streamlines this process, allowing for efficient iterations and evaluations. This function leverages the constructed sequential model and the defined cross-entropy loss to optimize the model's parameters. Additionally, incorporating an accuracy measure alongside the loss provides a more intuitive and comprehensive view of the model's performance, facilitating evaluations and adjustments during the development phase.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    - The 'train' function iterates over a specified number of epochs, and within each epoch, it further iterates over minibatches of the training data.\n",
    "    - For each minibatch, the function performs a forward pass through the model to obtain the scores, calculates the cross-entropy loss and performs backward propagation to update the model parameters using the specified learning rate.\n",
    "    - After processing each minibatch, the function calculates and prints the cost and accuracy for that epoch, providing real-time feedback on the model’s performance during training.\n",
    "    - The embedded 'accuracy' function calculates the model's accuracy by comparing the model’s predictions against the true labels over the validation data, providing an additional performance metric.\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "a9AFnkKpIE7h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size=128, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided training data, for a specified number\n",
    "    of epochs, minibatch size, and learning rate.\n",
    "\n",
    "    Args:\n",
    "      model (Sequential_layers): The neural network model to be trained.\n",
    "      epochs (int): The number of times the model should be trained over the\n",
    "                    entire dataset.\n",
    "      mb_size (int): The size of the minibatches used during training.\n",
    "                     Default is 128.\n",
    "      learning_rate (float): The learning rate used during training.\n",
    "                             Default is 1e-3.\n",
    "\n",
    "    Returns:\n",
    "      None: The function prints the cost and accuracy at the end of each epoch\n",
    "            but does not return any value.\n",
    "    \"\"\"\n",
    "    # The epoch is the number of times we want to go through the entire dataset\n",
    "    # to train our model\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            \n",
    "            # When calling the model function, sequence of Linear and activation\n",
    "            # functions (ReLU) selected with the Sequential class as the network\n",
    "            # architecture, are called on its feedforward function, giving as result\n",
    "            # the final probability of getting every possible output (each one of the\n",
    "            # numbers that represent the letters of ASL alphabet)\n",
    "            scores = model(x.T.view(np_tensor))\n",
    "\n",
    "            # Using softmax function, we calculate the gradient result (meaning the\n",
    "            # error on the last layer of the network compared to real values, the labels)\n",
    "            _, cost = softmaxXEntropy(scores, y)\n",
    "\n",
    "            # Call the backward propagation function in order to calculate gradients\n",
    "            # of every weight and bias\n",
    "            model.backward()\n",
    "\n",
    "            # Run the update function using the gradients calculated in order to update\n",
    "            # the model weights (Linear class W matrix values)\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        # Show the final cost result from the softmax function (after using the entire\n",
    "        # dataset) and the accuracy of the resulting model for that epoch using new\n",
    "        # values from validation set\n",
    "        print(f'Cost: {cost}, Accuracy: {accuracy(mb_size, x_val, y_val)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hik_FRd-IE7h"
   },
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkoKfLNmIE7h"
   },
   "source": [
    "And we are good to go!\n",
    "\n",
    "Here, we will finally create our x,y datasets, train, and validate the model based on a proposed architecture. In this case, we will use (784,1000) > ReLU > (1000, 1000) > ReLU > (1000, 1000) > ReLU (1000, 24). The 784 represents the pixels, the 24 represents the classes, and the rest are the hidden layers of our Neural Network. We will use a batch size of 512 and a learning rate of 1e-3 for 25 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRXqdQFCIE7i"
   },
   "source": [
    "\n",
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We will create a neural network architecture, train it and validate it.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    To solve the classification trouble of the ASL hand gestures to letters.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    We will use the trainning function, a sequential layer class with linear classes and relu corrections to compute weights and biases to transform the input vectors into predictions; the model will use the cross-entropy to measure the loss and to refine itself to produce a good accuracy. At the end, the model will be a trainned neural network capable of taking new data and create predictions.\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRjoIz9cIE7i",
    "outputId": "1adb593c-4154-4d7f-f1fe-5165214379af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Rows dropped from Train dataset with j or z labels: 1114\n",
      "Rows dropped from Validation dataset with j or z labels: 331\n",
      "------\n",
      "Train dataset shape: (26341, 785)\n",
      "Validation dataset shape: (6841, 785)\n",
      "------\n",
      "Train dataset shapes: x=(26341, 784) | y=(26341,)\n",
      "Validation dataset shapes: x=(3420, 784) | y=(3420,)\n",
      "Test dataset shapes: x=(3421, 784) | y=(3421,)\n"
     ]
    }
   ],
   "source": [
    "# Create an alphabet dict\n",
    "alphabet_dict = {letter: i for i, letter in enumerate(string.ascii_lowercase)}\n",
    "\n",
    "# Use the function to get the filtered datasets\n",
    "train_df, valid_df = filter_datasets(train_df_original, valid_df_original)\n",
    "\n",
    "# Calculate and print the number of rows dropped and the shape of datasets\n",
    "print('------')\n",
    "print(f'Rows dropped from Train dataset with j or z labels: {train_df_original.shape[0] - train_df.shape[0]}')\n",
    "print(f'Rows dropped from Validation dataset with j or z labels: {valid_df_original.shape[0] - valid_df.shape[0]}')\n",
    "print('------')\n",
    "print(f'Train dataset shape: {train_df.shape}')\n",
    "print(f'Validation dataset shape: {valid_df.shape}')\n",
    "print('------')\n",
    "\n",
    "# Specifying the target column to learn\n",
    "target = 'label'\n",
    "\n",
    "# Extracting the 'label' column and converting it to a numpy array\n",
    "y_train = np.array(train_df[target])\n",
    "y_val = np.array(valid_df[target])\n",
    "\n",
    "# Dropping the 'label' column from the original dataframes\n",
    "x_train = train_df.drop(columns=[target]).values.astype(np.float32)\n",
    "x_val = valid_df.drop(columns=[target]).values.astype(np.float32)\n",
    "\n",
    "# We get training set mean and standard deviation\n",
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "# Apply Z normalization to all datasets with trainig parameters\n",
    "x_train = normalize(x_mean, x_std, x_train)\n",
    "x_val = normalize(x_mean, x_std, x_val)\n",
    "\n",
    "# create datasets...\n",
    "x_val, y_val, x_test, y_test = split_val_test(x_val, y_val)\n",
    "print(f'Train dataset shapes: x={x_train.shape} | y={y_train.shape}')\n",
    "print(f'Validation dataset shapes: x={x_val.shape} | y={y_val.shape}')\n",
    "print(f'Test dataset shapes: x={x_test.shape} | y={y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "W0qqzsjAIE7i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters selection\n",
    "mb_size = 256\n",
    "learning_rate = 1e-3\n",
    "epochs = 25\n",
    "\n",
    "# Model architecture building\n",
    "model = Sequential_layers([\n",
    "    Linear(784, 1000),\n",
    "    ReLU(),\n",
    "    Linear(1000, 1000),\n",
    "    ReLU(),\n",
    "    Linear(1000, 1000),\n",
    "    ReLU(),\n",
    "    Linear(1000, 24)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "InOrmq7OIE7i",
    "outputId": "c32921d2-02f6-49bb-c211-cb026f420007",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.030891675663060357, Accuracy: 0.8108187134502924\n",
      "Cost: 0.007071395545862396, Accuracy: 0.8169590643274853\n",
      "Cost: 0.004528760212405083, Accuracy: 0.8187134502923976\n",
      "Cost: 0.002283353332567722, Accuracy: 0.8166666666666667\n",
      "Cost: 0.001873141804773087, Accuracy: 0.8166666666666667\n",
      "Cost: 0.0012804230535900295, Accuracy: 0.8184210526315789\n",
      "Cost: 0.0014776431697807694, Accuracy: 0.8201754385964912\n",
      "Cost: 0.0009632333619792692, Accuracy: 0.8210526315789474\n",
      "Cost: 0.0007656635320562428, Accuracy: 0.8216374269005848\n",
      "Cost: 0.000710647213535782, Accuracy: 0.822514619883041\n",
      "Cost: 0.0005994522763896873, Accuracy: 0.8210526315789474\n",
      "Cost: 0.0007118787283591264, Accuracy: 0.8219298245614035\n",
      "Cost: 0.0006886029768035182, Accuracy: 0.8210526315789474\n",
      "Cost: 0.0005094331987817239, Accuracy: 0.8222222222222222\n",
      "Cost: 0.00043986015941761033, Accuracy: 0.8228070175438597\n",
      "Cost: 0.00032441494841158795, Accuracy: 0.8230994152046783\n",
      "Cost: 0.0003364322774983779, Accuracy: 0.8230994152046783\n",
      "Cost: 0.0003447084231953909, Accuracy: 0.8230994152046783\n",
      "Cost: 0.00041724354401972345, Accuracy: 0.8228070175438597\n",
      "Cost: 0.00023431762785327328, Accuracy: 0.8236842105263158\n",
      "Cost: 0.00036458387244989697, Accuracy: 0.8228070175438597\n",
      "Cost: 0.00029815755584214717, Accuracy: 0.8236842105263158\n",
      "Cost: 0.0003277648903040776, Accuracy: 0.8242690058479533\n",
      "Cost: 0.00029468304278613245, Accuracy: 0.8239766081871345\n",
      "Cost: 0.0002678354306389711, Accuracy: 0.8230994152046783\n"
     ]
    }
   ],
   "source": [
    "# Use the train class to train the model architecture created using the training values\n",
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXCDLUsTIE7i"
   },
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTSAQSHBIE7i"
   },
   "source": [
    "Now we can compare our dataset against our test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjhk2IxQIE7i"
   },
   "source": [
    "<div style=\"border:2px solid #1E90FF; padding:15px; margin-top:5px; margin-bottom:5px; border-radius:15px; box-shadow:4px 4px 12px #aaa; background-color:#E6E6FA; font-family:'Comic Sans MS', 'Chalkboard SE', 'Comic Neue', cursive; color:#333;\">\n",
    "    <h3 style=\"margin-top:0;\">Code Documentation</h3>\n",
    "    <b>What:</b><br>\n",
    "    We will evaluate our model against unseen data.\n",
    "<br><br>\n",
    "    <b>Why:</b><br>\n",
    "    To assess the predictive accuracy and generalization capability of our ASL hand-gesture classification model.\n",
    "<br><br>\n",
    "    <b>How:</b><br>\n",
    "    We will introduce unseen data to determine the effectiveness of our model, both graphically, through direct plots, and programmatically, by utilizing the x_test and y_test datasets to calculate the overall accuracy on unseen data.\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eSS8pnnxIE7j",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Accuracy on test data====\n",
      "0.8123355743934522\n"
     ]
    }
   ],
   "source": [
    "# Get the model accuracy using the testing set\n",
    "print('===Accuracy on test data====')\n",
    "print(accuracy(mb_size,x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5hJ5zFEJ8as"
   },
   "source": [
    "We successfully constructed a fully connected Neural Network utilizing primarily numpy, achieving over 80% accuracy in classifying American Sign Language (ASL) hand gestures. The process involved creating layers of neurons, each with forward and backpropagation methods, and orchestrating these layers to communicate and function sequentially. We paid meticulous attention to data preparation, including normalization and selective exclusion of certain labels, and to dataset division for training, validation, and testing purposes.\n",
    "\n",
    "The model underwent a series of transformations and computations, with each result providing a loss that enhanced the model through backpropagation. We initiated the model with a set of values which were refined over time, emphasizing the importance of the training loop where we specified the number of epochs, the batch size, and the learning rate.\n",
    "\n",
    "The implemented ReLU layer worked effectively to introduce non-linearity, addressing the vanishing gradient problem, and the cost function, based on softmax of the output, provided a reliable measure of the model’s performance against known values.\n",
    "\n",
    "This endeavor has demonstrated the feasibility and efficacy of constructing a robust classification model with limited resources, underscoring the potential and versatility of foundational techniques in deep learning. While the model's accuracy is commendable, there remains room for optimization and enhancement, potentially exploring more sophisticated frameworks and methodologies in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuZ2oCA2IE7j"
   },
   "source": [
    "# References\n",
    "[1] Lingvano, “Sign language alphabet - tips and tricks to remember it fast!,” Lingvano ASL, 28-Feb-2023.  [Online]. Available: https://www.lingvano.com/asl/blog/sign-language-alphabet/. [Accessed: 29-Sep-2023]\n",
    "\n",
    "[2] V. Nair and G. E. Hinton, “Rectified Linear Units Improve Restricted Boltzmann Machines,” in Proceedings of the 27th International Conference on International Conference on Machine Learning, 2010, pp. 807–814.\n",
    "\n",
    "[3] K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” Dec. 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TBiNhrgIE7j"
   },
   "source": [
    "======================== End of Activity ========================"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
